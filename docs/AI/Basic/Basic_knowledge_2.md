线性模型（Linear Model）是机器学习和统计学习中最基础、最重要的一类模型。尽管其形式相对简单，但线性模型在理论研究和实际应用中都占有举足轻重的地位。

1 线性判别函数和决策边界
线性判别函数是线性分类模型的核心概念。本节将从二分类问题入手，逐步扩展到多分类问题，深入探讨线性分类的几何意义和数学性质。

1.1 二分类
二分类（Binary Classification）是最基本的分类问题，目标是将输入样本划分为两个互斥的类别。

线性判别函数的定义
给定D​维输入空间X=RD​，线性判别函数是一个从X​到R​的仿射映射：

f:RD→R,f(x)=w⊤x+b
其中：

x=[x1,x2,…,xD]⊤∈RD​ 是输入特征向量

w=[w1,w2,…,wD]⊤∈RD​ 是权重向量（weight vector）

b∈R​ 是偏置项（bias term），也称为截距（intercept）

展开形式：

f(x)=w1x1+w2x2+⋯+wDxD+b=∑d=1Dwdxd+b
几何解释：

权重向量w​决定了决策超平面的法向量方向

偏置项b​决定了超平面相对于原点的位置（平移量）

∥w∥​（权重向量的模）影响判别函数值的尺度

设D=2​，w=[2,1]⊤​，b=−3​，则判别函数为：

f(x1,x2)=2x1+x2−3
对于点(2,1)​：f(2,1)=2×2+1−3=2>0​，属于正类。
对于点(0,0)​：f(0,0)=−3<0​，属于负类。

增广表示
为了简化数学表达，我们通常采用增广表示（Augmented Representation）。

定义增广权重向量和增广特征向量：

w^=[wb]=[w1w2⋮wDb]∈RD+1
x^=[x1]=[x1x2⋮xD1]∈RD+1
则线性判别函数可简写为：

f(x)=w^⊤x^
验证：

w^⊤x^=[w1,w2,…,wD,b][x1x2⋮xD1]=∑d=1Dwdxd+b=w⊤x+b
增广表示的优点：

将仿射函数转化为线性函数，简化数学推导

便于矩阵运算和向量化实现

统一处理权重和偏置，代码实现更简洁

决策规则
基于线性判别函数，我们定义分类决策规则。

线性分类器的决策规则：

情形一：标签y∈{−1,+1}​

h(x)=sign(f(x))=sign(w⊤x+b)
其中符号函数（sign function）定义为：

sign(z)={+1,if z>00,if z=0−1,if z<0
或者简化为（将z=0​归为正类）：

sign(z)={+1,if z≥0−1,if z<0
情形二：标签y∈{0,1}​

h(x)=I(f(x)≥0)={1,if w⊤x+b≥00,if w⊤x+b<0
其中I(⋅)​是指示函数（indicator function）。

两种标签表示的转换：

从{0,1}​到{−1,+1}​：y′=2y−1​

从{−1,+1}​到{0,1}​：y′=(y+1)/2​

决策边界与决策区域
决策边界：决策边界（Decision Boundary）是使判别函数值为零的所有点构成的集合：

B={x∈RD∣f(x)=0}={x∈RD∣w⊤x+b=0}
几何性质：

在D​维空间中，决策边界是一个(D−1)​维的超平面（hyperplane）

当D=2​时，决策边界是一条直线

当D=3​时，决策边界是一个平面

超平面将RD​分成两个半空间

决策区域：决策边界将特征空间划分为两个决策区域：

正类区域：

R+1={x∈RD∣f(x)>0}
负类区域：

R−1={x∈RD∣f(x)<0}
决策区域的凸性：线性分类器的决策区域是凸集（convex set）。

证明：设x1,x2∈R+1​，则f(x1)>0​且f(x2)>0​。

对于任意λ∈[0,1]​，考虑凸组合xλ=λx1+(1−λ)x2​：

f(xλ)=w⊤(λx1+(1−λ)x2)+b
=λ(w⊤x1+b)+(1−λ)(w⊤x2+b)
=λf(x1)+(1−λ)f(x2)
由于λ≥0​，(1−λ)≥0​，f(x1)>0​，f(x2)>0​，所以：

f(xλ)=λf(x1)+(1−λ)f(x2)>0
因此xλ∈R+1​，证明R+1​是凸集。同理可证R−1​也是凸集。◻​

凸性的意义：

凸决策区域意味着不存在"孤岛"，同一类别的样本在特征空间中是"连通"的

这是线性分类器的局限性：无法处理类别区域非凸的情况（如XOR问题）

权重向量的几何意义
权重向量w​是决策超平面的法向量，指向正类区域。

证明：设x1,x2​是决策边界上的任意两点，则：

w⊤x1+b=0
w⊤x2+b=0
两式相减得：

w⊤(x1−x2)=0
这说明w​与决策边界上任意方向向量(x1−x2)​正交，因此w​是法向量。

对于f(x)>0​的点，有w⊤x>−b​，说明x​在沿w​方向上位于超平面的正侧。因此w​指向正类区域。◻​

几何直观：

想象站在决策边界上，面朝w​方向

前方是正类区域，后方是负类区域

w​的模∥w∥​不影响决策边界的方向，只影响判别函数值的尺度

样本点到决策边界的距离
点到超平面的距离公式：任意点x0∈RD​到超平面w⊤x+b=0​的距离为：

有符号距离：

dsigned(x0)=w⊤x0+b∥w∥2
无符号距离（绝对距离）：

d(x0)=|w⊤x0+b|∥w∥2
其中∥w∥2=∑d=1Dwd2​是权重向量的L2​范数（欧几里得范数）。

详细证明：

方法一：投影法

设xp​是x0​在超平面上的投影点（即最近点），则xp​满足两个条件：

xp​在超平面上：w⊤xp+b=0​

x0−xp​与超平面垂直，即平行于法向量w​

由条件2，存在λ∈R​使得：

x0−xp=λw
即：

xp=x0−λw
将xp​代入条件1：

w⊤(x0−λw)+b=0
w⊤x0−λw⊤w+b=0
w⊤x0+b=λ∥w∥22
λ=w⊤x0+b∥w∥22
x0​到超平面的距离为：

d=∥x0−xp∥2=∥λw∥2=|λ|⋅∥w∥2
=|w⊤x0+b∥w∥22|⋅∥w∥2=|w⊤x0+b|∥w∥2
◻​

方法二：解析几何法

超平面上任取一点x∗​，满足w⊤x∗+b=0​。

x0​到超平面的距离等于向量(x0−x∗)​在法向量方向上的投影长度：

d=|w⊤(x0−x∗)∥w∥2|=|w⊤x0−w⊤x∗∥w∥2|
由于w⊤x∗=−b​：

d=|w⊤x0+b∥w∥2|=|w⊤x0+b|∥w∥2
◻​

推论 3.1：原点到超平面w⊤x+b=0​的距离为：

dorigin=|b|∥w∥2
推论 3.2：如果∥w∥2=1​（单位法向量），则：

d(x0)=|w⊤x0+b|
即判别函数值的绝对值直接等于距离。

例 3.2：考虑二维空间中的超平面2x1+x2−2=0​。

权重向量：w=[2,1]⊤​

偏置：b=−2​

法向量的模：∥w∥2=4+1=5​

单位法向量：w^=15[2,1]⊤​

计算各点到超平面的距离：

点	f(x)​	距离	类别
(0,0)​	−2​	|−2|5=255≈0.894​	负类
(1,1)​	1​	|1|5=55≈0.447​	正类
(1,0)​	0​	0​	边界上
(2,2)​	4​	45=455≈1.789​	正类
函数间隔与几何间隔
在分类问题中，我们不仅关心样本是否被正确分类，还关心分类的置信度。

函数间隔：给定超平面(w,b)​和样本点(x,y)​，其中y∈{−1,+1}​，定义函数间隔（functional margin）为：

γ^=y⋅f(x)=y(w⊤x+b)
函数间隔的性质：

当γ^>0​时，样本被正确分类

当γ^<0​时，样本被错误分类

当γ^=0​时，样本在决策边界上

|γ^|​越大，分类的置信度越高

函数间隔的问题：函数间隔依赖于w​的尺度。如果将w​和b​同时乘以常数k>0​：

超平面不变（决策边界相同）

但函数间隔变为kγ^​

这意味着我们可以通过简单的缩放使函数间隔任意大，因此函数间隔不是一个好的度量。

几何间隔：定义几何间隔（geometric margin）为：

γ=y(w⊤x+b)∥w∥2=γ^∥w∥2
几何间隔的性质：

几何间隔等于样本点到超平面的有符号距离（带正负号）

几何间隔不随w​的尺度变化而变化

几何间隔具有明确的物理意义：正确分类时为正的距离，错误分类时为负的距离

验证尺度不变性：将(w,b)​替换为(kw,kb)​，k>0​：

γ′=y(kw⊤x+kb)∥kw∥2=k⋅y(w⊤x+b)|k|⋅∥w∥2=y(w⊤x+b)∥w∥2=γ
数据集的间隔：给定数据集D={(x(n),y(n))}n=1N​，定义数据集相对于超平面(w,b)​的间隔为所有样本几何间隔的最小值：

γD=minn=1,…,Nγ(n)=minn=1,…,Ny(n)(w⊤x(n)+b)∥w∥2
解释：数据集的间隔表示最难分类的样本（离决策边界最近的样本）到决策边界的距离。如果γD>0​，则所有样本都被正确分类。

线性可分性
线性可分：给定数据集D={(x(n),y(n))}n=1N​，其中y(n)∈{−1,+1}​。如果存在超平面(w,b)​使得：

y(n)(w⊤x(n)+b)>0,∀n=1,…,N
则称数据集是线性可分（linearly separable）的。

等价表述：

存在超平面能够将正类样本和负类样本完全分开

数据集的间隔γD>0​

凸包：点集S={x(1),…,x(m)}​的凸包（convex hull）定义为：

conv(S)={∑i=1mλix(i)∣λi≥0,∑i=1mλi=1}
即所有点的凸组合构成的集合。

线性可分的几何刻画：数据集D​线性可分，当且仅当正类样本集合的凸包与负类样本集合的凸包不相交。

conv({x(n)∣y(n)=+1})∩conv({x(n)∣y(n)=−1})=∅
线性不可分的例子——XOR问题：

考虑以下四个样本：

(0,0),y=−1​

(1,1),y=−1​

(0,1),y=+1​

(1,0),y=+1​

正类凸包是连接(0,1)​和(1,0)​的线段。
负类凸包是连接(0,0)​和(1,1)​的线段。
这两条线段在点(0.5,0.5)​相交，因此数据线性不可分。

这就是著名的XOR问题，它说明了单层感知器的局限性，也是促使多层神经网络发展的重要动力。

1.2 多分类
当类别数C>2​时，我们需要将二分类方法扩展到多分类问题。

多分类问题的形式化
问题设定：给定输入x∈RD​，预测其类别标签y∈{1,2,…,C}​。

方法分类：

分解法：将多分类问题分解为多个二分类问题

直接法：直接建立多类判别函数

一对其余（One-vs-Rest, OvR）
又称：One-vs-All (OvA)

基本思想：为每个类别c​训练一个二分类器，判断样本是否属于类别c​。

具体方法：

对于类别c=1,2,…,C​，构建训练集：

正类：所有标签为c​的样本，标记为+1​

负类：所有标签不为c​的样本，标记为−1​

训练得到C​个二分类器：

fc(x)=wc⊤x+bc,c=1,2,…,C
预测规则：

y^=arg⁡maxc∈{1,2,…,C}fc(x)
算法 3.1：OvR多分类

训练阶段：
for c = 1 to C do
    构建二分类数据集：
        正类：{x^(n) | y^(n) = c}
        负类：{x^(n) | y^(n) ≠ c}
    训练二分类器 f_c(x) = w_c^T x + b_c
end for
​
预测阶段：
输入：新样本 x
输出：ŷ = argmax_c f_c(x)
优点：

实现简单

只需训练C​个分类器，复杂度较低

每个分类器可以独立训练，便于并行化

缺点：

类别不平衡：每个二分类器中负类样本远多于正类（比例约为(C−1):1​）

分数不可比：不同分类器的输出可能不具有可比性

模糊区域：可能存在多个分类器都给出正预测或都给出负预测的情况

处理类别不平衡：

对负类进行下采样

对正类进行上采样

使用类别权重调整损失函数

一对一（One-vs-One, OvO）
基本思想：为每两个类别训练一个二分类器。

具体方法：

对于类别对(i,j)​，其中1≤i<j≤C​，构建训练集：

正类：所有标签为i​的样本

负类：所有标签为j​的样本

共训练：

(C2)=C(C−1)2
个二分类器。

预测规则（投票法）：

对于新样本x​，每个分类器fij​投票给预测的类别：

如果fij(x)>0​，类别i​获得一票

如果fij(x)<0​，类别j​获得一票

最终预测：

y^=arg⁡maxc∈{1,2,…,C}votes(c)
算法 3.2：OvO多分类

训练阶段：
for i = 1 to C-1 do
    for j = i+1 to C do
        构建二分类数据集：
            正类：{x^(n) | y^(n) = i}
            负类：{x^(n) | y^(n) = j}
        训练二分类器 f_{ij}(x)
    end for
end for
​
预测阶段：
输入：新样本 x
初始化：votes[1..C] = 0
for i = 1 to C-1 do
    for j = i+1 to C do
        if f_{ij}(x) > 0 then
            votes[i] += 1
        else
            votes[j] += 1
        end if
    end for
end for
输出：ŷ = argmax_c votes[c]
优点：

每个分类器只使用两个类别的数据，训练更快

类别平衡性较好

对于某些算法（如SVM），OvO的总训练时间可能比OvR更短

缺点：

需要训练O(C2)​个分类器，类别数多时存储开销大

预测时需要运行O(C2)​个分类器，预测较慢

投票可能产生平局

处理平局：

随机选择一个得票最多的类别

使用分类器输出的绝对值作为置信度进行加权投票

引入额外的规则打破平局

多类线性判别函数（直接法）
基本思想：直接定义C​个线性判别函数，每个类别一个。

模型定义：

fc(x)=wc⊤x+bc,c=1,2,…,C
矩阵形式：设：

W=[w1,w2,…,wC]∈RD×C
b=[b1,b2,…,bC]⊤∈RC
则所有判别函数值可以表示为：

f(x)=W⊤x+b∈RC
预测规则：

y^=arg⁡maxc∈{1,2,…,C}fc(x)
多类决策边界分析
类别i​和类别j​之间的决策边界是满足fi(x)=fj(x)​的点的集合：

Bij={x∣fi(x)=fj(x)}
={x∣(wi−wj)⊤x+(bi−bj)=0}
这仍然是一个超平面，法向量为wi−wj​。

推论：多类线性分类器的所有决策边界都是超平面，共有(C2)​个成对边界。

决策区域：类别c​的决策区域为：

Rc={x∣fc(x)>fj(x),∀j≠c}
=⋂j≠c{x∣(wc−wj)⊤x+(bc−bj)>0}
多类线性分类器的决策区域Rc​是凸多面体（convex polytope）。

证明：每个决策区域Rc​是C−1​个半空间的交集：

Rc=⋂j≠cHcj+
其中Hcj+={x∣(wc−wj)⊤x+(bc−bj)>0}​是一个开半空间。

半空间是凸集，凸集的交集仍是凸集，因此Rc​是凸集。

具体地，它是由有限个半空间界定的凸多面体。◻​

几何直观：

每个类别的决策区域是一个"楔形"或"锥形"区域

所有决策区域的并集覆盖整个特征空间

决策区域两两之间只通过边界相邻

参数冗余性
观察：在多类线性判别函数中，存在参数冗余。

平移不变性：如果将所有权重向量和偏置同时加上相同的量：

w~c=wc+v,b~c=bc+β,∀c
则分类决策不变。

证明：

f~c(x)=w~c⊤x+b~c
=(wc+v)⊤x+(bc+β)
=wc⊤x+bc+v⊤x+β
=fc(x)+(v⊤x+β)
由于v⊤x+β​对所有类别c​相同，因此：

arg⁡maxcf~c(x)=arg⁡maxc[fc(x)+(v⊤x+β)]=arg⁡maxcfc(x)
消除冗余的方法：

方法一：设置某一类（通常是最后一类C​）的参数为零：

wC=0,bC=0
这样只需学习C−1​个权重向量和偏置，总参数量从C(D+1)​减少到(C−1)(D+1)​。

方法二：添加约束条件：

∑c=1Cwc=0,∑c=1Cbc=0
在Softmax回归中的应用：Softmax回归中常用方法一，将最后一类的参数设为零作为参考类别。

2 Logistic回归
Logistic回归（Logistic Regression）是一种广泛使用的线性分类模型。尽管名称中包含"回归"，但它实际上是一种分类方法。Logistic回归的核心思想是使用Sigmoid函数将线性判别函数的输出转换为概率值，从而实现概率化的分类预测。

为什么不能直接用线性函数分类？
考虑二分类问题，标签y∈{0,1}​。如果直接使用线性函数f(x)=w⊤x+b​预测y​，存在以下问题：

问题一：输出范围不受限

线性函数f(x)​可以是任意实数(−∞,+∞)​

但标签y​只能是0​或1​

无法直接将线性输出解释为类别

问题二：无法给出概率解释

我们希望模型输出"样本属于某类的概率"

概率值必须在[0,1]​范围内

线性函数无法满足这一要求

问题三：对离群点敏感

如果使用最小二乘法拟合

远离决策边界的点会对决策边界产生过大影响

导致分类效果不佳

解决方案：引入一个将R​映射到(0,1)​的激活函数，将线性输出转换为概率。

激活函数的选择
理想的激活函数σ:R→(0,1)​应满足：

值域为(0,1)​：输出可解释为概率

单调递增：输入越大，正类概率越高

平滑可微：便于使用梯度优化方法

计算简单：便于实际应用

Sigmoid函数完美满足这些要求。

2.1 Sigmoid函数
定义与基本形式
Sigmoid函数：Sigmoid函数，也称为Logistic函数或S形函数，定义为：

σ(z)=11+e−z
等价形式：

σ(z)=ez1+ez=ezez+1
验证等价性：

11+e−z=11+e−z⋅ezez=ezez+1
图像特征
Sigmoid函数的图像呈S形曲线：

特征	描述
定义域	(−∞,+∞)​
值域	(0,1)​（开区间，不包含0和1）
单调性	严格单调递增
对称中心	点(0,0.5)​
z=0​处的值	σ(0)=0.5​
左极限	limz→−∞σ(z)=0​
右极限	limz→+∞σ(z)=1​
拐点	z=0​，此处曲率最大
重要性质
对称性/互补性：

σ(−z)=1−σ(z)
详细证明：

σ(−z)=11+e−(−z)=11+ez
1−σ(z)=1−11+e−z=1+e−z−11+e−z=e−z1+e−z
=e−z1+e−z⋅ezez=1ez+1=11+ez
因此σ(−z)=1−σ(z)​。

几何意义：Sigmoid曲线关于点(0,0.5)​中心对称。

导数的优美形式：

σ′(z)=σ(z)(1−σ(z))
详细证明：

使用商法则，设u=1​，v=1+e−z​：

σ′(z)=ddz(11+e−z)=−1(1+e−z)2⋅ddz(1+e−z)
=−1(1+e−z)2⋅(−e−z)=e−z(1+e−z)2
将其改写为σ(z)​和1−σ(z)​的乘积：

σ(z)=11+e−z
1−σ(z)=e−z1+e−z
σ(z)(1−σ(z))=11+e−z⋅e−z1+e−z=e−z(1+e−z)2
因此σ′(z)=σ(z)(1−σ(z))​。

导数的性质：

σ′(z)>0​对所有z​成立（单调递增）

σ′(z)​的最大值在z=0​处取得：σ′(0)=0.5×0.5=0.25​

当|z|​很大时，σ′(z)≈0​（梯度消失）

逆函数——logit函数：

Sigmoid函数的逆函数称为logit函数：

σ−1(p)=logit(p)=log⁡p1−p
其中p∈(0,1)​。

验证：设p=σ(z)​，则：

p1−p=σ(z)1−σ(z)=11+e−ze−z1+e−z=1e−z=ez
log⁡p1−p=log⁡ez=z
因此logit(σ(z))=z​。◻​

与双曲正切函数的关系：

σ(z)=1+tanh⁡(z/2)2
或等价地：

tanh⁡(z)=2σ(2z)−1
2.2 Logistic回归模型
模型定义
Logistic回归模型：Logistic回归模型定义样本x​属于类别1的条件概率为：

P(y=1∣x;w,b)=σ(w⊤x+b)=11+e−(w⊤x+b)
相应地，属于类别0的概率为：

P(y=0∣x;w,b)=1−σ(w⊤x+b)=e−(w⊤x+b)1+e−(w⊤x+b)
统一表达式（伯努利分布形式）：

P(y∣x;w,b)=σ(w⊤x+b)y⋅(1−σ(w⊤x+b))1−y
其中y∈{0,1}​。

简化记号：令y^=σ(w⊤x+b)​表示预测的正类概率，令z=w⊤x+b​，则：

y^=σ(z)=P(y=1∣x)
P(y∣x)=y^y(1−y^)1−y
决策规则
概率阈值决策：

y^class={1,if P(y=1∣x)≥θ0,if P(y=1∣x)<θ
其中θ∈(0,1)​是决策阈值，通常取θ=0.5​。

当θ=0.5​时：

P(y=1∣x)≥0.5⇔σ(z)≥0.5⇔z≥0⇔w⊤x+b≥0
因此决策规则等价于：

y^class=I(w⊤x+b≥0)
这与线性分类器的决策规则一致。

2.3 对数几率（Log-Odds）解释
Logistic回归有一个非常优雅的概率解释，揭示了模型的本质。

几率与对数几率
几率：事件发生的几率（odds）定义为该事件发生概率与不发生概率之比：

odds=P(y=1∣x)P(y=0∣x)=P(y=1∣x)1−P(y=1∣x)
几率的直观理解：

odds=1​：正负类等可能

odds=2​：正类概率是负类的2倍（正类概率23​）

odds=9​：正类概率是负类的9倍（正类概率910​）

对数几率：对数几率（log-odds），也称为logit，是几率的自然对数：

logit(p)=log⁡p1−p
其中p=P(y=1∣x)​。

核心定理
Logistic回归的对数几率线性性：在Logistic回归模型中，对数几率是输入特征的线性函数：

log⁡P(y=1∣x)P(y=0∣x)=w⊤x+b
详细证明：

设p=P(y=1∣x)=σ(z)​，其中z=w⊤x+b​。

log⁡p1−p=log⁡σ(z)1−σ(z)
代入σ(z)=11+e−z​和1−σ(z)=e−z1+e−z​：

=log⁡11+e−ze−z1+e−z=log⁡1e−z=log⁡ez=z=w⊤x+b
参数的解释
定理 3.9（权重的对数几率解释）：权重wd​表示特征xd​增加一个单位时，对数几率的变化量。

证明：设x′=x+ed​，其中ed​是第d​个坐标为1、其余为0的单位向量。则：

log⁡P(y=1∣x′)P(y=0∣x′)−log⁡P(y=1∣x)P(y=0∣x)
=(w⊤x′+b)−(w⊤x+b)=w⊤(x′−x)=w⊤ed=wd
几率比解释：特征xd​增加一个单位时，几率变为原来的ewd​倍。

证明：

odds′odds=exp⁡(log⁡odds′−log⁡odds)=exp⁡(wd)=ewd
医学诊断：假设Logistic回归模型预测患病概率，特征x1​表示年龄（岁），对应权重w1=0.05​。

对数几率解释：年龄每增加1岁，患病的对数几率增加0.05

几率比解释：年龄每增加1岁，患病几率变为原来的e0.05≈1.051​倍，即增加约5.1%

10年效应：年龄增加10岁，几率变为原来的e0.5≈1.649​倍，即增加约65%

2.4 广义线性模型视角
Logistic回归可以从广义线性模型（Generalized Linear Model, GLM）的框架来理解。

GLM的三要素
广义线性模型由三个部分组成：

1. 随机成分（Random Component）：响应变量y​服从指数族分布。

2. 系统成分（Systematic Component）：线性预测子η=w⊤x+b​。

3. 连接函数（Link Function）：将期望μ=E[y∣x]​与线性预测子联系起来：g(μ)=η​。

Logistic回归作为GLM
对于Logistic回归：

随机成分：y∣x∼Bernoulli(p)​，即伯努利分布，其中p=P(y=1∣x)​。

伯努利分布的概率质量函数：

P(y∣p)=py(1−p)1−y,y∈{0,1}
可以写成指数族形式：

P(y∣p)=exp⁡(ylog⁡p1−p+log⁡(1−p))
系统成分：η=w⊤x+b​

连接函数：logit连接

g(μ)=logit(μ)=log⁡μ1−μ
由于μ=E[y]=p​，连接函数将概率p​映射到线性预测子η​。

典则连接：logit是伯努利分布的典则连接函数（canonical link function），这使得Logistic回归具有一些优良的统计性质。

2.5 参数学习
最大似然估计框架
给定训练数据集D={(x(n),y(n))}n=1N​，其中y(n)∈{0,1}​，我们使用最大似然估计（Maximum Likelihood Estimation, MLE）来学习参数(w,b)​。

假设：样本独立同分布（i.i.d.）。

似然函数的推导
似然函数：参数的似然函数定义为观测数据出现的概率：

L(w,b)=P(D∣w,b)=∏n=1NP(y(n)∣x(n);w,b)
代入Logistic回归的概率模型：

L(w,b)=∏n=1Ny^(n)y(n)(1−y^(n))1−y(n)
其中y^(n)=σ(w⊤x(n)+b)​。

展开形式：

L(w,b)=∏n:y(n)=1y^(n)⋅∏n:y(n)=0(1−y^(n))
对数似然函数
取对数得到对数似然函数（log-likelihood）：

ℓ(w,b)=log⁡L(w,b)
=∑n=1N[y(n)log⁡y^(n)+(1−y(n))log⁡(1−y^(n))]
记号简化：令z(n)=w⊤x(n)+b​，则y^(n)=σ(z(n))​。

利用log⁡σ(z)=z−log⁡(1+ez)=−log⁡(1+e−z)​和log⁡(1−σ(z))=−log⁡(1+ez)​：

ℓ(w,b)=∑n=1N[−y(n)log⁡(1+e−z(n))−(1−y(n))log⁡(1+ez(n))]
进一步简化：注意log⁡(1+ez)=z+log⁡(1+e−z)​，可得：

ℓ(w,b)=∑n=1N[y(n)z(n)−log⁡(1+ez(n))]
交叉熵损失函数
交叉熵：对于两个概率分布p​（真实分布）和q​（预测分布），它们的交叉熵（cross-entropy）定义为：

H(p,q)=−∑xp(x)log⁡q(x)=−Ep[log⁡q]
对于二分类问题：

真实分布p​：P(y=1)=y​，P(y=0)=1−y​（one-hot编码）

预测分布q​：P(y=1)=y^​，P(y=0)=1−y^​

二元交叉熵损失：单个样本的二元交叉熵损失（Binary Cross-Entropy Loss，BCE）定义为：

LBCE(y^,y)=−[ylog⁡y^+(1−y)log⁡(1−y^)]
数据集上的平均损失：

L(w,b)=1N∑n=1NLBCE(y^(n),y(n))
=−1N∑n=1N[y(n)log⁡y^(n)+(1−y(n))log⁡(1−y^(n))]
关系：

L(w,b)=−1Nℓ(w,b)
因此：

arg⁡maxw,bℓ(w,b)=arg⁡minw,bL(w,b)
最大化对数似然 ⇔​ 最小化交叉熵损失

梯度的详细推导
为了使用梯度下降法优化参数，我们需要计算损失函数（或对数似然函数）关于参数的梯度。

设z=w⊤x+b​，y^=σ(z)​，则：

∂y^∂z=y^(1−y^)
∂z∂w=x,∂z∂b=1
对数似然函数关于参数的梯度为：

∂ℓ∂w=∑n=1N(y(n)−y^(n))x(n)
∂ℓ∂b=∑n=1N(y(n)−y^(n))
详细证明：

对于单个样本(x,y)​，其对数似然为：

ℓ=ylog⁡y^+(1−y)log⁡(1−y^)
Step 1：计算∂ℓ∂y^​

∂ℓ∂y^=yy^−1−y1−y^
通分：

=y(1−y^)−(1−y)y^y^(1−y^)=y−yy^−y^+yy^y^(1−y^)=y−y^y^(1−y^)
Step 2：计算∂y^∂z​

由Sigmoid函数的导数性质：

∂y^∂z=σ′(z)=σ(z)(1−σ(z))=y^(1−y^)
Step 3：应用链式法则计算∂ℓ∂z​

∂ℓ∂z=∂ℓ∂y^⋅∂y^∂z=y−y^y^(1−y^)⋅y^(1−y^)=y−y^
关键观察：梯度形式非常简洁！∂ℓ∂z=y−y^​，即真实标签与预测概率之差。

Step 4：计算∂ℓ∂w​和∂ℓ∂b​

∂ℓ∂w=∂ℓ∂z⋅∂z∂w=(y−y^)x
∂ℓ∂b=∂ℓ∂z⋅∂z∂b=(y−y^)⋅1=y−y^
Step 5：对所有样本求和

∂ℓ∂w=∑n=1N(y(n)−y^(n))x(n)
∂ℓ∂b=∑n=1N(y(n)−y^(n))
交叉熵损失函数的梯度为（带负号和1/N​因子）：

∂L∂w=−1N∑n=1N(y(n)−y^(n))x(n)=1N∑n=1N(y^(n)−y(n))x(n)
∂L∂b=1N∑n=1N(y^(n)−y(n))
向量化形式：设X∈RN×D​为数据矩阵，y∈RN​为标签向量，y^∈RN​为预测概率向量，则：

∂L∂w=1NX⊤(y^−y)
梯度下降算法
批量梯度下降（Batch Gradient Descent, BGD）：

每次使用全部样本计算梯度并更新参数。

更新规则（最大化对数似然）：

w←w+η∑n=1N(y(n)−y^(n))x(n)
b←b+η∑n=1N(y(n)−y^(n))
其中η>0​是学习率。

Logistic回归的批量梯度下降

输入：训练集 D = {(x^(n), y^(n))}_{n=1}^N
      学习率 η > 0
      最大迭代次数 T
      收敛阈值 ε > 0
输出：参数 w, b
​
1.  初始化：w ← 0 ∈ R^D，b ← 0
2.  for t = 1 to T do
3.      // 前向传播：计算预测概率
4.      for n = 1 to N do
5.          z^(n) ← w^T x^(n) + b
6.          ŷ^(n) ← σ(z^(n)) = 1/(1 + exp(-z^(n)))
7.      end for
8.
9.      // 计算梯度
10.     g_w ← Σ_{n=1}^N (y^(n) - ŷ^(n)) x^(n)
11.     g_b ← Σ_{n=1}^N (y^(n) - ŷ^(n))
12.
13.     // 更新参数（梯度上升，最大化对数似然）
14.     w ← w + η · g_w
15.     b ← b + η · g_b
16.
17.     // 检查收敛
18.     if ||g_w||_2 < ε and |g_b| < ε then
19.         break
20.     end if
21. end for
22. return w, b
随机梯度下降（Stochastic Gradient Descent, SGD）：

每次只使用一个随机样本更新参数。

w←w+η(y(n)−y^(n))x(n)
b←b+η(y(n)−y^(n))
优点：

每次更新计算量小，O(D)​

可以处理大规模数据

引入噪声有助于跳出局部最优

缺点：

收敛不稳定，需要精心调节学习率

难以利用向量化加速

小批量梯度下降（Mini-batch Gradient Descent）：

折中方案，每次使用一小批样本（batch）。

设批量大小为B​，第t​次迭代选取的小批量索引集合为Bt​：

w←w+ηB∑n∈Bt(y(n)−y^(n))x(n)
典型批量大小：32, 64, 128, 256

牛顿法与IRLS
由于Logistic回归的损失函数是凸的且二阶可微，可以使用牛顿法（Newton's Method）加速收敛。

Hessian矩阵：

对数似然函数关于w​的Hessian矩阵为：

H=∂2ℓ∂w∂w⊤=−∑n=1Ny^(n)(1−y^(n))x(n)x(n)⊤
推导：

∂∂wj(∂ℓ∂wi)=∂∂wj(∑n(y(n)−y^(n))xi(n))
=−∑nxi(n)∂y^(n)∂wj=−∑nxi(n)y^(n)(1−y^(n))xj(n)
矩阵形式：

H=−X⊤SX
其中S=diag(y^(1)(1−y^(1)),…,y^(N)(1−y^(N)))​是对角矩阵。

牛顿更新：

w←w−H−1∇wℓ
=w+(X⊤SX)−1X⊤(y−y^)
IRLS算法：迭代重加权最小二乘（Iteratively Reweighted Least Squares）是牛顿法在Logistic回归中的具体实现形式。

定义加权响应变量：

z=Xw+S−1(y−y^)
则牛顿更新等价于求解加权最小二乘问题：

wnew=(X⊤SX)−1X⊤Sz
收敛性：牛顿法通常具有二次收敛速度，比梯度下降快得多，但每步计算量更大。

2.6 正则化
为了防止过拟合，通常在损失函数中加入正则化项。

L2正则化（Ridge）
定义：L2正则化在损失函数中加入权重的平方和：

Lreg(w,b)=L(w,b)+λ2∥w∥22
=−1N∑n=1N[y(n)log⁡y^(n)+(1−y(n))log⁡(1−y^(n))]+λ2∑d=1Dwd2
其中λ>0​是正则化系数（超参数）。

梯度：

∂Lreg∂w=∂L∂w+λw
更新规则：

w←w−η(∂L∂w+λw)=w(1−ηλ)−η∂L∂w
注意因子(1−ηλ)​，这就是权重衰减（weight decay）。

效果：

倾向于使权重较小

防止某些特征的权重过大

提高模型的泛化能力

L1正则化（Lasso）
定义：

Lreg(w,b)=L(w,b)+λ∥w∥1=L(w,b)+λ∑d=1D|wd|
效果：

产生稀疏解，部分权重恰好为零

实现自动特征选择

在wd=0​处不可微，需要使用次梯度或坐标下降

弹性网（Elastic Net）
定义：结合L1和L2正则化：

Lreg(w,b)=L(w,b)+λ1∥w∥1+λ22∥w∥22
优点：

兼具L1的稀疏性和L2的稳定性

当特征高度相关时表现更好

2.7 Logistic回归的理论性质
凸性
Logistic回归的交叉熵损失函数是关于参数(w,b)​的凸函数。

证明：只需证明Hessian矩阵是半正定的。

H=∂2L∂w∂w⊤=1N∑n=1Ny^(n)(1−y^(n))x(n)x(n)⊤
对任意向量v∈RD​：

v⊤Hv=1N∑n=1Ny^(n)(1−y^(n))(v⊤x(n))2
由于：

y^(n)∈(0,1)​

因此y^(n)(1−y^(n))>0​

(v⊤x(n))2≥0​

所以v⊤Hv≥0​，Hessian矩阵半正定，损失函数是凸函数。

局部最优解就是全局最优解

梯度下降法保证收敛到全局最优

解的唯一性
如果数据矩阵X​列满秩（即特征之间线性无关），则加了L2正则化的Logistic回归的最优解唯一。

注意：不加正则化时，如果数据线性可分，最优解可能不唯一（参数可以无限增大）。

3 Softmax回归
Softmax回归（Softmax Regression），又称多项Logistic回归（Multinomial Logistic Regression）或多类Logistic回归，是Logistic回归在多分类问题上的自然推广。

3.1 从二分类到多分类
问题设定
多分类问题：给定输入x∈RD​，预测其类别标签y∈{1,2,…,C}​，其中C>2​是类别数。

目标：学习一个模型，输出样本属于每个类别的概率分布：

P(y=c∣x),c=1,2,…,C
约束：输出必须是有效的概率分布

P(y=c∣x)≥0,∀c​

∑c=1CP(y=c∣x)=1​

推广思路
在Logistic回归中，我们使用Sigmoid函数将单个线性输出转换为概率。对于多分类：

需要C​个线性函数，每个类别一个

需要一个函数将C​个实数转换为概率分布

Softmax函数正是这样的函数。

3.2 Softmax函数
定义
Softmax函数：Softmax函数将C​维向量z=[z1,z2,…,zC]⊤∈RC​映射为概率分布：

softmax(z)c=exp⁡(zc)∑j=1Cexp⁡(zj),c=1,2,…,C
向量形式：

softmax(z)=exp⁡(z)1⊤exp⁡(z)=1∑j=1Cezj[ez1ez2⋮ezC]
其中exp⁡(z)​表示逐元素指数运算。

重要性质
概率分布性：Softmax的输出是有效的概率分布。

证明：

非负性：exp⁡(zc)>0​，且分母∑jexp⁡(zj)>0​，因此softmax(z)c>0​

归一化：∑c=1Csoftmax(z)c=∑c=1Cexp⁡(zc)∑j=1Cexp⁡(zj)=∑c=1Cexp⁡(zc)∑j=1Cexp⁡(zj)=1​

平移不变性：对任意常数a∈R​：

softmax(z+a1)=softmax(z)
证明：

softmax(z+a1)c=exp⁡(zc+a)∑j=1Cexp⁡(zj+a)=eaexp⁡(zc)ea∑j=1Cexp⁡(zj)=exp⁡(zc)∑j=1Cexp⁡(zj)
应用：在数值计算中，为避免指数溢出，通常减去最大值：

softmax(z)=softmax(z−maxjzj⋅1)
与argmax的关系：

arg⁡maxcsoftmax(z)c=arg⁡maxczc
Softmax保持了元素的相对大小关系。

温度参数：带温度参数T>0​的Softmax：

softmax(z/T)c=exp⁡(zc/T)∑j=1Cexp⁡(zj/T)
当T→0+​时，Softmax趋近于argmax（硬选择）

当T→+∞​时，Softmax趋近于均匀分布

T=1​是标准Softmax

Softmax的导数
Softmax的Jacobian矩阵：设p=softmax(z)​，则：

∂pi∂zj={pi(1−pi),if i=j−pipj,if i≠j
矩阵形式：

∂p∂z=diag(p)−pp⊤
证明：

当i=j​时：

∂pi∂zi=∂∂zi(ezi∑kezk)
使用商法则：

=ezi⋅∑kezk−ezi⋅ezi(∑kezk)2=ezi∑kezk−e2zi(∑kezk)2
=pi−pi2=pi(1−pi)
当i≠j​时：

∂pi∂zj=∂∂zj(ezi∑kezk)=0−ezi⋅ezj(∑kezk)2=−pipj
3.3 Softmax回归模型
模型定义
Softmax回归模型：Softmax回归为每个类别定义一个线性函数：

zc=wc⊤x+bc,c=1,2,…,C
然后通过Softmax函数转换为概率：

P(y=c∣x;W,b)=softmax(z)c=exp⁡(wc⊤x+bc)∑j=1Cexp⁡(wj⊤x+bj)
参数：

权重矩阵：W=[w1,w2,…,wC]∈RD×C​

偏置向量：b=[b1,b2,…,bC]⊤∈RC​

总参数数量：(D+1)×C​

矩阵形式：

z=W⊤x+b∈RC
p=softmax(z)∈RC
3.3.3.2 参数冗余与消除
由于Softmax的平移不变性，参数存在冗余。

消除冗余：设wC=0​，bC=0​（以第C​类为参考类）。

则：

P(y=c∣x)=exp⁡(wc⊤x+bc)1+∑j=1C−1exp⁡(wj⊤x+bj),c=1,…,C−1
P(y=C∣x)=11+∑j=1C−1exp⁡(wj⊤x+bj)
参数数量减少为(D+1)×(C−1)​。

与Logistic回归的关系：当C=2​时，消除冗余后的Softmax回归退化为Logistic回归：

P(y=1∣x)=exp⁡(w1⊤x+b1)1+exp⁡(w1⊤x+b1)=σ(w1⊤x+b1)
3.4 参数学习
标签编码
One-hot编码：将类别标签y∈{1,2,…,C}​编码为向量y∈{0,1}C​：

y=[y1,y2,…,yC]⊤,yc={1,if y=c0,otherwise
例如：对于3分类问题，类别2的one-hot编码为y=[0,1,0]⊤​。

交叉熵损失函数
多类交叉熵损失：单个样本的交叉熵损失：

L(p,y)=−∑c=1Cyclog⁡pc=−y⊤log⁡p
由于y​是one-hot向量，只有一个分量为1（设为第c∗​类），因此：

L(p,y)=−log⁡pc∗=−log⁡P(y=c∗∣x)
这就是负对数似然（Negative Log-Likelihood, NLL）。

数据集上的平均损失：

L(W,b)=−1N∑n=1N∑c=1Cyc(n)log⁡pc(n)
=−1N∑n=1Nlog⁡P(y=c(n)∣x(n))
其中c(n)​是第n​个样本的真实类别。

梯度推导
交叉熵损失关于参数的梯度为：

∂L∂wc=1N∑n=1N(pc(n)−yc(n))x(n)
∂L∂bc=1N∑n=1N(pc(n)−yc(n))
简洁形式：对所有类别：

∂L∂W=1NX⊤(P−Y)
其中X∈RN×D​是数据矩阵，P,Y∈RN×C​分别是预测概率矩阵和标签矩阵。

证明（单样本情况）：

设ℓ=−∑cyclog⁡pc​，pc=softmax(z)c​，zc=wc⊤x+bc​。

Step 1：计算∂ℓ∂zk​

∂ℓ∂zk=−∑cyc1pc∂pc∂zk
利用Softmax的导数：

∂pc∂zk=pc(δck−pk)
其中δck​是Kronecker delta。

∂ℓ∂zk=−∑cycpc(δck−pk)pc=−∑cyc(δck−pk)
=−yk+pk∑cyc=−yk+pk⋅1=pk−yk
Step 2：计算∂ℓ∂wc​和∂ℓ∂bc​

∂ℓ∂wc=∂ℓ∂zc⋅∂zc∂wc=(pc−yc)x
∂ℓ∂bc=∂ℓ∂zc⋅∂zc∂bc=pc−yc
观察：梯度形式与Logistic回归完全一致！这体现了两者的统一性。

梯度下降更新
wc←wc−ηN∑n=1N(pc(n)−yc(n))x(n)
bc←bc−ηN∑n=1N(pc(n)−yc(n))
4 感知器
感知器（Perceptron）是由Frank Rosenblatt于1958年提出的一种最简单的神经网络模型，是神经网络和深度学习的历史起点。

4.1 感知器模型
模型定义
感知器：感知器是一个二分类模型，其决策函数为：

h(x)=sign(f(x))=sign(w⊤x+b)
其中：

w∈RD​是权重向量

b∈R​是偏置

标签y∈{−1,+1}​

神经元视角：感知器可以看作一个人工神经元

输入：x=[x1,…,xD]⊤​

加权求和：z=∑d=1Dwdxd+b​

激活函数：h=sign(z)​

输出：h∈{−1,+1}​

几何解释
感知器定义了一个超平面w⊤x+b=0​：

超平面一侧的点被分为正类

另一侧的点被分为负类

权重向量w​是超平面的法向量，指向正类区域

4.2 感知器学习算法
损失函数
感知器使用的损失函数是感知器损失：

L(w,b)=−∑x(n)∈My(n)(w⊤x(n)+b)
其中M​是所有被错误分类的样本集合。

性质分析：

对于正确分类的样本：y(n)(w⊤x(n)+b)>0​，不计入损失

对于错误分类的样本：y(n)(w⊤x(n)+b)<0​，损失为正

损失函数非负，且仅当所有样本被正确分类时为零

梯度计算
对于单个错误分类样本(x,y)​，损失为−y(w⊤x+b)​。

∂L∂w=−yx
∂L∂b=−y
感知器学习算法
算法 3.4：感知器学习算法（原始形式）

输入：训练集 D = {(x^(n), y^(n))}_{n=1}^N，其中 y^(n) ∈ {-1, +1}
      学习率 η > 0（通常取 η = 1）
      最大迭代轮数 T_max
输出：参数 w, b
​
1.  初始化：w ← 0 ∈ R^D，b ← 0
2.  for epoch = 1 to T_max do
3.      误分类样本数 ← 0
4.      for n = 1 to N do
5.          if y^(n)(w^T x^(n) + b) ≤ 0 then    // 样本被错误分类
6.              w ← w + η y^(n) x^(n)            // 更新权重
7.              b ← b + η y^(n)                  // 更新偏置
8.              误分类样本数 ← 误分类样本数 + 1
9.          end if
10.     end for
11.     if 误分类样本数 = 0 then
12.         break    // 所有样本正确分类，算法收敛
13.     end if
14. end for
15. return w, b
更新规则的直观理解：

当样本(x,y)​被错误分类时：

情况1：y=+1​但w⊤x+b<0​（正类样本被误判为负类）

更新：w←w+x​

效果：使w⊤x​增大，推动决策边界使该样本落入正类区域

情况2：y=−1​但w⊤x+b>0​（负类样本被误判为正类）

更新：w←w−x​

效果：使w⊤x​减小，推动决策边界使该样本落入负类区域

4.3 感知器的收敛性
感知器收敛定理是感知器算法最重要的理论结果。

收敛定理
感知器收敛定理，Novikoff, 1962：设训练数据集D​是线性可分的，存在超平面w∗⋅x+b∗=0​能够将正负样本完全分开。设：

R=maxn∥x^(n)∥2
是增广样本向量的最大模长，

γ=minny(n)(w∗⊤x(n)+b∗)∥w^∗∥2
是数据集的几何间隔。

则感知器算法的误分类更新次数k​满足：

k≤(Rγ)2
详细证明
为简化证明，使用增广表示：w^=[w;b]​，x^=[x;1]​。

设w^∗​是使数据线性可分的最优增广权重向量，且∥w^∗∥2=1​。

设γ=minny(n)w^∗⊤x^(n)>0​（归一化的几何间隔）。

设R=maxn∥x^(n)∥2​。

设算法在第k​次更新后的参数为w^(k)​，初始w^(0)=0​。

Step 1：分析w^(k)⊤w^∗​的下界

每次更新时，设当前误分类样本为(x^(n),y(n))​：

w^(k)=w^(k−1)+y(n)x^(n)
计算与w^∗​的内积：

w^(k)⊤w^∗=w^(k−1)⊤w^∗+y(n)x^(n)⊤w^∗
≥w^(k−1)⊤w^∗+γ
递推得：

w^(k)⊤w^∗≥kγ
Step 2：分析∥w^(k)∥22​的上界

∥w^(k)∥22=∥w^(k−1)+y(n)x^(n)∥22
=∥w^(k−1)∥22+2y(n)w^(k−1)⊤x^(n)+∥x^(n)∥22
由于样本被误分类：y(n)w^(k−1)⊤x^(n)≤0​

∥w^(k)∥22≤∥w^(k−1)∥22+R2
递推得：

∥w^(k)∥22≤kR2
Step 3：综合两个不等式

由Cauchy-Schwarz不等式：

w^(k)⊤w^∗≤∥w^(k)∥2⋅∥w^∗∥2=∥w^(k)∥2
结合Step 1和Step 2：

kγ≤w^(k)⊤w^∗≤∥w^(k)∥2≤kR2=kR
kγ≤kR
k≤(Rγ)2
收敛性的意义与局限
意义：

对于线性可分数据，算法保证在有限步内收敛

收敛速度与数据的间隔γ​成反比：间隔越大，收敛越快

收敛速度与样本的范围R​成正比：样本范围越大，收敛越慢

局限：

线性不可分：算法无法收敛，会在不同解之间震荡

解不唯一：最终解依赖于样本的遍历顺序和初始化

无法输出概率：只能给出硬分类结果

4.4 参数平均感知器
为了提高感知器的泛化能力，Collins (2002) 提出了平均感知器算法。

基本思想
不使用最后一轮迭代的参数，而是使用训练过程中所有参数的平均值：

w¯=1T∑t=1Tw(t)
b¯=1T∑t=1Tb(t)
其中T​是总的参数状态数（或更新次数）。

直观理解：平均化相当于对参数进行投票，减少了对单个误分类样本的过度响应，提高了模型的稳定性。

高效实现
直接计算平均值需要存储所有中间参数，空间复杂度为O(TD)​。

累积技巧：维护一个累积向量wsum​：

每次更新后：

wsum←wsum+w
最后：

w¯=wsumT
更高效的实现：考虑参数在多次迭代中保持不变的情况。

4.5 多分类感知器
模型定义
对于C​类分类问题，定义C​个权重向量w1,w2,…,wC​。

预测规则：

y^=arg⁡maxc∈{1,…,C}(wc⊤x+bc)
更新规则
当样本(x,y)​被错误分类为y^≠y​时：

（
增
加
正
确
类
别
的
得
分
）
wy←wy+ηx（增加正确类别的得分）
（
减
少
错
误
预
测
类
别
的
得
分
）
wy^←wy^−ηx（减少错误预测类别的得分）
5 支持向量机
支持向量机（Support Vector Machine, SVM）是由Vapnik等人在1990年代提出的一种强大的监督学习算法。SVM的核心思想是寻找具有最大间隔的分类超平面，并通过核技巧处理非线性可分问题。

5.1 最大间隔分类器
动机
对于线性可分数据，存在无穷多个超平面能够正确分类所有样本。哪个超平面最好？

直观想法：选择"最中间"的超平面，即离最近样本点距离最大的超平面。

原因：

最大间隔超平面对噪声更鲁棒

具有更好的泛化能力（由统计学习理论保证）

间隔的定义
回顾前面的定义：

几何间隔：样本(x(n),y(n))​到超平面(w,b)​的几何间隔：

γ(n)=y(n)(w⊤x(n)+b)∥w∥2
数据集的间隔：

γ=minnγ(n)
最大间隔优化问题
原始问题：

maxw,bγ
s.t.y(n)(w⊤x(n)+b)∥w∥2≥γ,n=1,…,N
标准化：由于w​的尺度是自由的，我们可以设γ^=γ∥w∥2=1​（函数间隔为1）。

则γ=1∥w∥2​，最大化γ​等价于最小化∥w∥2​。

标准形式：

minw,b12∥w∥22
s.t.y(n)(w⊤x(n)+b)≥1,n=1,…,N
这是一个凸二次规划（Convex Quadratic Programming, QP）问题：

目标函数是凸二次函数

约束是线性不等式

存在唯一的全局最优解

5.2 对偶问题与支持向量
拉格朗日函数
引入拉格朗日乘子αn≥0​，n=1,…,N​：

L(w,b,α)=12∥w∥22−∑n=1Nαn[y(n)(w⊤x(n)+b)−1]
原始问题等价于：

minw,bmaxα≥0L(w,b,α)
对偶问题推导
Step 1：对w​求偏导并令其为0

∂L∂w=w−∑n=1Nαny(n)x(n)=0
⇒w=∑n=1Nαny(n)x(n)
Step 2：对b​求偏导并令其为0

∂L∂b=−∑n=1Nαny(n)=0
⇒∑n=1Nαny(n)=0
Step 3：代入拉格朗日函数

将w=∑nαny(n)x(n)​代入：

L=12‖∑nαny(n)x(n)‖2−∑nαny(n)w⊤x(n)−b∑nαny(n)+∑nαn
由于∑nαny(n)=0​，b​项消失。

展开第一项：

12∑n∑mαnαmy(n)y(m)x(n)⊤x(m)
第二项：

∑nαny(n)(∑mαmy(m)x(m))⊤x(n)=∑n∑mαnαmy(n)y(m)x(n)⊤x(m)
因此：

L=∑nαn−12∑n∑mαnαmy(n)y(m)x(n)⊤x(m)
对偶问题：

maxα∑n=1Nαn−12∑n=1N∑m=1Nαnαmy(n)y(m)x(n)⊤x(m)
s.t.αn≥0,n=1,…,N
∑n=1Nαny(n)=0
KKT条件
最优解必须满足Karush-Kuhn-Tucker (KKT) 条件：

原始可行性：y(n)(w⊤x(n)+b)≥1​

对偶可行性：αn≥0​

互补松弛条件：

αn[y(n)(w⊤x(n)+b)−1]=0
互补松弛条件的含义：对于每个样本n​：

要么αn=0​

要么y(n)(w⊤x(n)+b)=1​（样本恰好落在间隔边界上）

支持向量
支持向量：满足αn>0​的样本称为支持向量（Support Vector）。

由KKT条件，支持向量满足：

y(n)(w⊤x(n)+b)=1
即支持向量恰好落在间隔边界上（距离超平面的距离为1/∥w∥​）。

重要性质：

最优超平面完全由支持向量决定

非支持向量对决策边界没有影响

支持向量通常只占所有样本的一小部分

决策函数：

f(x)=w⊤x+b=∑n=1Nαny(n)x(n)⊤x+b
由于非支持向量的αn=0​：

f(x)=∑n∈SVαny(n)x(n)⊤x+b
其中SV={n∣αn>0}​是支持向量的索引集合。

5.3 核函数与核技巧
非线性可分问题
当数据在原始空间中线性不可分时，可以通过特征映射将数据映射到高维空间，使其在高维空间中线性可分。

特征映射：ϕ:RD→H​

其中H​是高维（可能无限维）的特征空间或希尔伯特空间。

例子：二维到六维的映射：

x=[x1,x2]⊤↦ϕ(x)=[1,2x1,2x2,x12,x22,2x1x2]⊤
在高维空间中，SVM的对偶问题变为：

maxα∑n=1Nαn−12∑n=1N∑m=1Nαnαmy(n)y(m)ϕ(x(n))⊤ϕ(x(m))
核技巧
核心观察：对偶问题只涉及样本之间的内积ϕ(x)⊤ϕ(x′)​。

核函数：核函数κ:RD×RD→R​定义为特征空间中的内积：

κ(x,x′)=ϕ(x)⊤ϕ(x′)
核技巧（Kernel Trick）：如果我们能够直接计算κ(x,x′)​，而无需显式计算ϕ(x)​，就可以避免在高维空间中的显式计算。

例子：多项式核

κ(x,x′)=(x⊤x′+1)2
展开验证（D=2​）：

(x⊤x′+1)2=(x1x1′+x2x2′+1)2
=1+2x1x1′+2x2x2′+x12x1′2+x22x2′2+2x1x2x1′x2′
=[1,2x1,2x2,x12,x22,2x1x2]⋅[1,2x1′,2x2′,x1′2,x2′2,2x1′x2′]⊤
=ϕ(x)⊤ϕ(x′)
计算复杂度：

显式计算ϕ(x)⊤ϕ(x′)​：O(D2)​

使用核函数：O(D)​

常用核函数
1. 线性核（Linear Kernel）：

κ(x,x′)=x⊤x′
对应恒等映射ϕ(x)=x​，即原始空间中的线性SVM。

2. 多项式核（Polynomial Kernel）：

κ(x,x′)=(x⊤x′+c)d
其中c≥0​是常数，d∈N​是多项式次数。

d=1​, c=0​：线性核

d=2​：二次核，能够捕捉二阶特征交互

3. 高斯核/RBF核（Gaussian/Radial Basis Function Kernel）：

κ(x,x′)=exp⁡(−∥x−x′∥22σ2)=exp⁡(−γ∥x−x′∥2)
其中σ>0​是带宽参数，γ=12σ2​。

重要性质：高斯核对应无限维的特征空间。

证明（简略）：利用Taylor展开：

exp⁡(−∥x−x′∥22σ2)=exp⁡(−∥x∥22σ2)exp⁡(−∥x′∥22σ2)exp⁡(x⊤x′σ2)
将exp⁡(x⊤x′σ2)​展开为无穷级数，可以得到无限维的特征映射。

4. Sigmoid核（Sigmoid Kernel）：

κ(x,x′)=tanh⁡(βx⊤x′+θ)
其中β>0​，θ<0​。

注意：Sigmoid核在某些参数下不满足Mercer条件。

Mercer条件
Mercer定理：一个函数κ(⋅,⋅)​是有效的核函数（即存在对应的特征映射ϕ​），当且仅当对任意样本集{x(1),…,x(N)}​，核矩阵K​是半正定的：

Knm=κ(x(n),x(m))
K⪰0
5.4 软间隔SVM
动机
在实际应用中，数据往往不是严格线性可分的：

可能存在噪声和异常值

硬间隔SVM会试图拟合所有样本，容易过拟合

软间隔（Soft Margin）允许一些样本违反间隔约束。

松弛变量
引入松弛变量（slack variable）ξn≥0​：

y(n)(w⊤x(n)+b)≥1−ξn
松弛变量的含义：

ξn=0​：样本在间隔边界上或外部（正确分类且满足间隔约束）

0<ξn<1​：样本在间隔内但被正确分类

ξn=1​：样本恰好在决策边界上

ξn>1​：样本被错误分类

软间隔SVM优化问题
minw,b,ξ12∥w∥22+C∑n=1Nξn
s.t.y(n)(w⊤x(n)+b)≥1−ξn,n=1,…,N
ξn≥0,n=1,…,N
参数C​的作用：

C​是正则化参数，控制间隔最大化与错误分类惩罚之间的权衡

C​大：更不容忍错误，间隔小，可能过拟合

C​小：更容忍错误，间隔大，可能欠拟合

对偶问题
软间隔SVM的对偶问题为：

maxα∑n=1Nαn−12∑n=1N∑m=1Nαnαmy(n)y(m)κ(x(n),x(m))
s.t.0≤αn≤C,n=1,…,N
∑n=1Nαny(n)=0
与硬间隔SVM相比，唯一的区别是αn​有了上界C​（box constraint）。

KKT条件分析：

αn=0​：样本在间隔外，不是支持向量

0<αn<C​：样本在间隔边界上，ξn=0​

αn=C​：样本在间隔内或被错误分类，ξn>0​

Hinge损失
软间隔SVM等价于最小化正则化的Hinge损失：

minw,b12∥w∥22+C∑n=1Nmax(0,1−y(n)(w⊤x(n)+b))
Hinge损失定义为：

ℓhinge(y,f(x))=max(0,1−yf(x))=[1−yf(x)]+
其中[z]+=max(0,z)​是ReLU函数。

6 损失函数对比
不同的线性分类模型采用了不同的损失函数。本节系统比较这些损失函数的性质。

6.1 统一框架
设z=y⋅f(x)=y(w⊤x+b)​为样本的函数间隔。

各种损失函数都可以写成ℓ(z)​的形式。

6.2 常见损失函数详解
损失
ℓ0−1(z)=I(z≤0)={1,if z≤00,if z>0
性质：

直接度量分类错误

非凸、不连续、不可微

NP难优化问题

实际中用其他损失函数作为代理（surrogate）

感知器损失
ℓperceptron(z)=max(0,−z)=[−z]+
性质：

凸函数

连续但在z=0​处不可微

只惩罚错误分类的样本

是0-1损失的松弛

Hinge损失（SVM）
ℓhinge(z)=max(0,1−z)=[1−z]+
性质：

凸函数

连续但在z=1​处不可微

不仅要求正确分类，还要求有间隔（z≥1​）

是0-1损失的紧致凸上界

Logistic损失
ℓlogistic(z)=log⁡(1+e−z)
性质：

凸函数

处处光滑可微

渐近行为：

z→+∞​：ℓ→0​

z→−∞​：ℓ≈−z​（线性增长）

指数损失（AdaBoost）
ℓexp(z)=e−z
性质：

凸函数

处处光滑可微

对离群点非常敏感（指数增长）

是AdaBoost算法对应的损失函数

6.3 损失函数比较表
损失函数	公式	凸性	光滑性	离群点敏感度	对应模型
0-1损失	I(z≤0)​	非凸	不连续	低	理想分类器
感知器损失	[−z]+​	凸	非光滑	低	感知器
Hinge损失	[1−z]+​	凸	非光滑	中等	SVM
Logistic损失	log⁡(1+e−z)​	凸	光滑	中等	Logistic回归
指数损失	e−z​	凸	光滑	高	AdaBoost
6.4 损失函数的关系
感知器损失是Hinge损失向左平移1个单位的结果

当z→+∞​时（正确分类且置信度高）：

Hinge损失和感知器损失都趋近于0

Logistic损失和指数损失也趋近于0

当z→−∞​时（错误分类且置信度高）：

Hinge损失和感知器损失线性增长

Logistic损失也线性增长

指数损失指数增长（对离群点非常敏感）

凸上界关系：

Hinge损失、Logistic损失、指数损失都是0-1损失的凸上界

最小化这些代理损失可以间接最小化0-1损失