深度神经网络的训练本质上是一个高维非凸优化问题，其成功在很大程度上依赖于优化算法的选择与正则化策略的设计。优化算法决定了模型能否高效地收敛到损失曲面上的良好解，而正则化技术则确保模型在训练数据之外仍具有良好的泛化能力。本章系统讨论深度学习中的优化方法论——从损失曲面的结构分析到现代自适应优化器的设计——以及参数初始化、数据预处理、归一化技术、超参数调优和正则化方法等关键实践技术。

## 1 网络优化

### 1.1 网络结构多样性

深度神经网络的损失函数是关于模型参数的高度复杂映射，其优化难度与网络结构密切相关。不同的网络架构（全连接、卷积、循环、残差等）定义了不同的参数空间几何结构，进而影响损失曲面的拓扑性质。

网络结构多样性带来的一个重要现象是参数空间的对称性（symmetry）。以全连接网络为例，交换同一隐藏层内任意两个神经元的所有入边和出边权重，网络的输入-输出映射完全不变。对于拥有 $n$ 个隐藏单元的单隐藏层网络，这种置换对称性意味着至少存在 $n!$ 个等价的全局最优解。这一观察表明，深度网络的损失曲面天然具有高度的非唯一性，优化的目标不应是寻找某个特定的最优参数配置，而是找到损失足够低且泛化性能良好的参数区域。

此外，现代深度网络普遍处于过参数化（over-parameterized）状态——参数数量远超训练样本数量。过参数化虽然看似加剧了过拟合风险，但近年来的理论研究揭示了一系列反直觉的现象：过参数化网络的损失曲面更加平滑，局部极小值倾向于也是全局极小值（Allen-Zhu et al., 2019），且梯度下降在此类网络上可以隐式地实现正则化（即倾向于找到范数较小的解）。

### 1.2 高维变量的非凸优化

深度网络的损失函数 $\mathcal{L}(\boldsymbol{\theta})$ 关于参数 $\boldsymbol{\theta} \in \mathbb{R}^d$ 通常是非凸的，这使得基于梯度的优化方法面临诸多挑战。

**临界点的分类**：损失曲面上的临界点（$\nabla \mathcal{L} = \mathbf{0}$）可以通过Hessian矩阵 $\mathbf{H} = \nabla^2 \mathcal{L}$ 的特征值来分类。若所有特征值为正，则为局部极小值；若所有特征值为负，则为局部极大值；若特征值同时存在正负，则为鞍点（saddle point）。

**高维空间中鞍点占主导**：Dauphin等人（2014）的分析表明，在高维空间中，鞍点的数量以指数级别压倒局部极小值。直觉是：在 $d$ 维空间的随机临界点处，每个Hessian特征值为正或负的概率各约50%，因此全部为正（局部极小值）的概率仅为 $2^{-d}$。对于 $d$ 为数百万甚至数十亿的深度网络，鞍点几乎是唯一可能遇到的临界点类型。

**平坦区域与锐利极小值**：损失曲面还存在大范围的平坦区域（plateau），在这些区域中梯度接近零，导致优化进展极其缓慢。此外，Hessian矩阵特征值的分布（锐利极小值 vs 平坦极小值）与模型的泛化性能相关——Keskar等人（2017）的实验表明，大批量训练倾向于收敛到损失曲面的锐利极小值，而小批量训练更可能找到平坦极小值，后者通常具有更好的泛化能力。

### 1.3 神经网络优化的改善方法

针对上述优化困难，研究者从多个角度提出了改善策略，主要包括：

**更好的优化算法**：从基础的随机梯度下降到带动量的SGD、自适应学习率方法（AdaGrad、RMSProp、Adam等），优化器的进化显著提升了训练效率和稳定性（详见7.2节）。

**合理的参数初始化**：良好的初始化可以确保前向传播和反向传播过程中信号和梯度的幅值保持稳定，避免训练初期的梯度消失或爆炸（详见7.3节）。

**数据预处理**：对输入数据进行标准化和白化可以改善损失曲面的条件数，加速优化收敛（详见7.4节）。

**逐层归一化**：Batch Normalization、Layer Normalization等技术通过规范化中间层的激活分布，减少内部协变量偏移（internal covariate shift），使得训练更加稳定并允许使用更大的学习率（详见7.5节）。

**架构设计**：残差连接（skip connection）、稠密连接（DenseNet）等架构创新通过提供梯度直通路径来改善深层网络的优化景观。

## 2 优化算法

### 2.1 小批量梯度下降

小批量梯度下降（Mini-batch Gradient Descent）是深度学习中最基础的优化范式。在每次迭代中，从训练集 $\mathcal{D}$ 中随机采样一个小批量 $\mathcal{B} = \{(\mathbf{x}_i, y_i)\}_{i=1}^{B}$，基于该子集估计梯度并更新参数：

$$
\boldsymbol{\theta}^{(t+1)} = \boldsymbol{\theta}^{(t)} - \eta \cdot \frac{1}{B} \sum_{i \in \mathcal{B}} \nabla_{\boldsymbol{\theta}} \mathcal{L}_i(\boldsymbol{\theta}^{(t)})
$$

其中 $\eta$ 为学习率，$B$ 为批量大小。小批量梯度下降在计算效率与梯度估计质量之间取得了平衡：相比全批量梯度下降（$B = N$），它显著降低了每次迭代的计算开销；相比随机梯度下降（$B = 1$），它通过批量内平均降低了梯度估计的方差。

梯度估计的方差为 $\text{Var}[\hat{\mathbf{g}}] = \frac{\sigma^2}{B}$，其中 $\sigma^2$ 为单样本梯度的方差。这意味着梯度估计的标准差以 $1/\sqrt{B}$ 的速率衰减——将批量大小翻倍仅使标准差降低约30%，因此存在收益递减效应。

### 2.2 批量大小选择

批量大小 $B$ 是一个影响训练效率、收敛速度和泛化性能的关键超参数。

**小批量**（$B = 32 \sim 256$）：梯度估计噪声较大，但这种噪声可以充当隐式正则化，帮助模型逃离损失曲面的锐利极小值，从而收敛到更平坦、泛化能力更强的区域。此外，小批量的每次迭代计算开销低，在固定时间预算内可以执行更多的参数更新步骤。

**大批量**（$B = 1024 \sim 65536$）：梯度估计更准确，每步的更新方向更可靠，可以更充分地利用GPU的并行计算能力。然而，大批量训练面临泛化性能下降的风险（Keskar et al., 2017）。

**线性缩放规则**（Goyal et al., 2017）：当批量大小增大 $k$ 倍时，学习率也应相应增大 $k$ 倍，以保持参数更新幅度的一致性。但这一规则在极大批量下会失效，需要配合学习率预热（warmup）策略——在训练初期使用较小的学习率逐渐增大到目标值。

**梯度累积**（Gradient Accumulation）：当单卡显存无法容纳所需的大批量时，可以在多个小批量上依次计算梯度并累加，最终执行一次参数更新，等效地实现大批量训练。

### 2.3 学习率调整

学习率 $\eta$ 是深度学习中最重要的超参数。过大的学习率导致训练振荡甚至发散，过小的学习率则导致收敛缓慢或陷入次优解。实践中通常采用学习率调度策略（learning rate schedule），在训练过程中动态调整学习率。

**分段常数衰减（Step Decay）**：在预设的训练轮次（milestone）处将学习率乘以衰减因子 $\gamma$（通常取0.1）：

$$
\eta_t = \eta_0 \cdot \gamma^{\lfloor t / T_{\text{step}} \rfloor}
$$

这是最简单也最广泛使用的策略之一，ResNet的原始训练即采用此方案。

**指数衰减**：

$$
\eta_t = \eta_0 \cdot \gamma^t, \quad 0 < \gamma < 1
$$

**余弦退火（Cosine Annealing）**（Loshchilov & Hutter, 2017）：

$$
\eta_t = \eta_{\min} + \frac{1}{2}(\eta_0 - \eta_{\min})\left(1 + \cos\frac{t \pi}{T}\right)
$$

余弦退火在现代训练流程中极为流行，它提供了一种平滑的学习率衰减曲线。配合周期性重启（warm restarts），余弦退火可以帮助优化器周期性地逃离局部极小值。

**线性预热（Linear Warmup）**：在训练的前 $T_w$ 步线性增大学习率：

$$
\eta_t = \eta_0 \cdot \frac{t}{T_w}, \quad t \leq T_w
$$

预热策略在大批量训练和Transformer模型的训练中尤为重要，可以防止训练初期由于参数远离最优解而导致的大梯度引发的不稳定。

### 2.4 梯度估计修正

标准SGD的一个局限是所有参数共享同一个标量学习率，且梯度方向可能在高曲率方向上振荡。以下方法通过修正梯度估计来改善优化过程。

**动量法（Momentum）**（Polyak, 1964）：引入历史梯度的指数加权移动平均来平滑更新方向，抑制振荡并加速沿一致方向的运动：

$$
\mathbf{v}^{(t)} = \beta \mathbf{v}^{(t-1)} + \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}^{(t)})
$$

$$
\boldsymbol{\theta}^{(t+1)} = \boldsymbol{\theta}^{(t)} - \eta \mathbf{v}^{(t)}
$$

动量系数 $\beta$ 通常取0.9。从物理学角度理解，$\mathbf{v}^{(t)}$ 类似于"速度"，梯度类似于"加速度"，$\beta$ 对应摩擦系数。动量法使得参数更新具有惯性——即使当前梯度为零（如鞍点或平坦区域），累积的动量仍可以推动参数继续移动。

**Nesterov加速梯度（NAG）**（Nesterov, 1983）：先根据当前动量进行"前瞻"（lookahead），然后在前瞻位置计算梯度：

$$
\mathbf{v}^{(t)} = \beta \mathbf{v}^{(t-1)} + \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}^{(t)} - \eta \beta \mathbf{v}^{(t-1)})
$$

$$
\boldsymbol{\theta}^{(t+1)} = \boldsymbol{\theta}^{(t)} - \eta \mathbf{v}^{(t)}
$$

NAG的"前瞻"机制可以理解为一种预测性修正——它预见了动量将要带来的更新，并提前对此进行调整，从而在目标函数变化方向上更加敏锐，减少过冲现象。

**AdaGrad**（Duchi et al., 2011）：为每个参数维护独立的自适应学习率，累计历史梯度的平方和进行缩放：

$$
\mathbf{G}^{(t)} = \mathbf{G}^{(t-1)} + \mathbf{g}_t \odot \mathbf{g}_t
$$

$$
\boldsymbol{\theta}^{(t+1)} = \boldsymbol{\theta}^{(t)} - \frac{\eta}{\sqrt{\mathbf{G}^{(t)} + \epsilon}} \odot \mathbf{g}_t
$$

其中 $\mathbf{g}_t = \nabla_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}^{(t)})$，$\epsilon \approx 10^{-8}$ 为数值稳定常数。AdaGrad对频繁更新的参数自动降低学习率、对稀疏更新的参数保持较大学习率，在处理稀疏梯度（如NLP中的词嵌入训练）时表现出色。然而，其累积求和导致分母单调递增，后期学习率趋近于零，训练过早停滞。

**RMSProp**（Hinton, 2012）：使用指数移动平均替代AdaGrad的累积求和来解决学习率单调衰减问题：

$$
\mathbf{G}^{(t)} = \beta \mathbf{G}^{(t-1)} + (1 - \beta) \mathbf{g}_t \odot \mathbf{g}_t
$$

$$
\boldsymbol{\theta}^{(t+1)} = \boldsymbol{\theta}^{(t)} - \frac{\eta}{\sqrt{\mathbf{G}^{(t)} + \epsilon}} \odot \mathbf{g}_t
$$

$\beta$ 通常取0.9。RMSProp仅保留近期梯度的信息，使学习率能够自适应地增大或缩小。

**Adam**（Adaptive Moment Estimation）（Kingma & Ba, 2015）：结合了动量法（一阶矩估计）和RMSProp（二阶矩估计）的优点：

$$
\mathbf{m}^{(t)} = \beta_1 \mathbf{m}^{(t-1)} + (1 - \beta_1) \mathbf{g}_t
$$

$$
\mathbf{v}^{(t)} = \beta_2 \mathbf{v}^{(t-1)} + (1 - \beta_2) \mathbf{g}_t \odot \mathbf{g}_t
$$

$$
\hat{\mathbf{m}}^{(t)} = \frac{\mathbf{m}^{(t)}}{1 - \beta_1^t}, \quad \hat{\mathbf{v}}^{(t)} = \frac{\mathbf{v}^{(t)}}{1 - \beta_2^t}
$$

$$
\boldsymbol{\theta}^{(t+1)} = \boldsymbol{\theta}^{(t)} - \frac{\eta}{\sqrt{\hat{\mathbf{v}}^{(t)}} + \epsilon} \odot \hat{\mathbf{m}}^{(t)}
$$

其中 $\beta_1 = 0.9$，$\beta_2 = 0.999$ 为默认超参数，偏差修正项 $1/(1-\beta^t)$ 补偿了移动平均在训练初期的低估偏差。Adam因其对学习率的鲁棒性和在多数任务上的良好表现，已成为深度学习的默认优化器。

**AdamW**（Loshchilov & Hutter, 2019）修正了Adam中权重衰减的实现方式——将权重衰减从梯度计算中解耦，直接在参数更新中施加：

$$
\boldsymbol{\theta}^{(t+1)} = (1 - \lambda\eta)\boldsymbol{\theta}^{(t)} - \frac{\eta}{\sqrt{\hat{\mathbf{v}}^{(t)}} + \epsilon} \odot \hat{\mathbf{m}}^{(t)}
$$

AdamW在Transformer的训练中已成为事实上的标准。

### 2.5 优化算法小结

各优化算法可按两个维度进行分类：是否使用动量（一阶矩估计）和是否使用自适应学习率（二阶矩估计）。

|            | 无自适应学习率    | 自适应学习率     |
| ---------- | ----------------- | ---------------- |
| **无动量** | SGD               | AdaGrad, RMSProp |
| **有动量** | SGD+Momentum, NAG | Adam, AdamW      |

选择建议：对于计算机视觉任务（尤其是CNN），带动量的SGD配合余弦退火学习率调度往往能取得最佳泛化性能，但需要精细的学习率调优；对于NLP任务（尤其是Transformer），AdamW是默认选择，通常配合线性预热和余弦/线性衰减的学习率调度；对于快速原型验证和初步实验，Adam以其对超参数的鲁棒性提供了最佳的"开箱即用"体验。

## 3 参数初始化

参数初始化是深度网络训练的起点，其质量直接影响训练的收敛速度和最终性能。不恰当的初始化可能导致信号在前向传播中逐层爆炸或消失，梯度在反向传播中相应地爆炸或消失。

**基本原则**：权重应打破对称性（不能全部初始化为相同的值，否则同一层的所有神经元将永远保持一致），同时保持各层输出的方差大致稳定。偏置通常初始化为零。

### 3.1 基于固定方差的参数初始化

最简单的初始化策略是将权重从固定方差的分布中随机采样。

**高斯初始化**：$w_{ij} \sim \mathcal{N}(0, \sigma^2)$，其中 $\sigma$ 为预设的标准差。若 $\sigma$ 过大，激活值将迅速饱和（对于Sigmoid/Tanh）或爆炸（对于ReLU）；若 $\sigma$ 过小，激活值将逐层衰减至零。

**均匀初始化**：$w_{ij} \sim U(-a, a)$，其方差为 $a^2/3$。

这些简单策略在浅层网络中可能有效，但对于深层网络，固定方差无法保证信号在逐层传播中保持稳定。

### 3.2 基于方差缩放的参数初始化

方差缩放初始化（Variance Scaling Initialization）的核心思想是：根据每一层的输入和输出维度自适应地设置权重方差，使得各层的激活值和梯度在统计意义上保持大致恒定的方差。

**Xavier初始化**（Glorot & Bengio, 2010）：假设激活函数为线性（或在零附近的线性近似区域），要求前向传播中各层激活值的方差保持不变，同时反向传播中梯度的方差也保持不变。联合这两个条件，得到：

$$
w_{ij} \sim \mathcal{N}\left(0, \frac{2}{n_{in} + n_{out}}\right) \quad \text{或} \quad w_{ij} \sim U\left(-\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}}\right)
$$

其中 $n_{in}$ 和 $n_{out}$ 分别为该层的输入和输出神经元数量。Xavier初始化适用于Sigmoid和Tanh激活函数。

**He初始化**（He et al., 2015）：专为ReLU及其变体设计。由于ReLU将约50%的激活值置零，方差在每层减半，因此需要将权重方差增大一倍来补偿：

$$
w_{ij} \sim \mathcal{N}\left(0, \frac{2}{n_{in}}\right)
$$

He初始化是使用ReLU激活函数的深层网络（如ResNet）的标准选择。对于Leaky ReLU（负斜率为 $\alpha$），方差调整为 $2/(1+\alpha^2) \cdot n_{in}$。

**推导逻辑**：以He初始化为例，考虑单层变换 $\mathbf{y} = \text{ReLU}(\mathbf{W}\mathbf{x})$。假设 $\mathbf{x}$ 的各分量独立同分布且均值为零，$\mathbf{W}$ 的各元素独立同分布且均值为零，则 $\text{Var}[y_j] = \frac{1}{2} n_{in} \cdot \text{Var}[w] \cdot \text{Var}[x]$（其中 $1/2$ 因子来自ReLU将负半轴置零）。令 $\text{Var}[y] = \text{Var}[x]$，解得 $\text{Var}[w] = 2/n_{in}$。

### 3.3 正交初始化

正交初始化（Orthogonal Initialization）（Saxe et al., 2014）将权重矩阵初始化为正交矩阵（或其缩放版本），即 $\mathbf{W}^\top \mathbf{W} = \mathbf{I}$。对于非方阵，使用SVD分解获取其正交基。

正交矩阵的所有奇异值均为1，这保证了前向传播中信号的范数不会因矩阵乘法而改变（不放大也不缩小），从而实现了各层激活值方差的精确保持。对于线性网络（无激活函数），Saxe等人证明正交初始化可以实现训练动态的精确解析，表明各层以不同速率收敛。

正交初始化在循环神经网络中尤为重要——由于循环权重矩阵 $\mathbf{W}_h$ 在时间维度上被反复应用，其特征值决定了梯度的长期行为。正交初始化确保 $\mathbf{W}_h$ 的特征值模为1，有效缓解了RNN中的梯度消失/爆炸问题。

## 4 数据预处理

输入数据的统计特性对优化效率有深远影响。当输入特征的尺度差异悬殊时，损失曲面呈现为高度各向异性的椭球形等高线，标准梯度下降将在不同方向上以不同速率收敛，导致训练效率低下。

**标准化（Normalization / Standardization）**：将每个特征维度独立地缩放为零均值、单位方差：

$$
\hat{x}_i = \frac{x_i - \mu_i}{\sigma_i}
$$

其中 $\mu_i$ 和 $\sigma_i$ 分别为第 $i$ 个特征在训练集上的均值和标准差。标准化使各特征处于同一量纲和尺度，改善了损失曲面的条件数。

**PCA白化（PCA Whitening）**：在标准化的基础上，进一步消除特征之间的相关性。对中心化后的数据协方差矩阵 $\boldsymbol{\Sigma}$ 进行特征值分解 $\boldsymbol{\Sigma} = \mathbf{U} \mathbf{\Lambda} \mathbf{U}^\top$，白化变换为：

$$
\hat{\mathbf{x}} = \mathbf{\Lambda}^{-1/2} \mathbf{U}^\top (\mathbf{x} - \boldsymbol{\mu})
$$

白化后的数据协方差矩阵为单位矩阵 $\mathbf{I}$。白化将损失曲面的等高线从椭球形转化为球形，使得梯度方向直接指向最优解，理论上可以显著加速收敛。然而，白化的计算开销较高（需要计算 $d \times d$ 的协方差矩阵和特征值分解），且对于图像等高维数据可能不切实际。

在深度学习实践中，逐层归一化（7.5节）可以看作是对隐藏层激活值进行动态的、近似的白化操作，从而将预处理的思想推广到网络的每一层。

## 5 逐层归一化

逐层归一化（Layer-wise Normalization）是深度学习中最重要的训练技巧之一。其核心思想是：在每一层的线性变换之后、非线性激活之前（或之后），对该层的净输入（或激活）进行归一化处理，使其分布保持稳定。

### 5.1 批量归一化

批量归一化（Batch Normalization, BN）（Ioffe & Szegedy, 2015）是最早被广泛采用的逐层归一化方法。

**动机**：Ioffe和Szegedy提出了"内部协变量偏移"（Internal Covariate Shift, ICS）的概念——由于前层参数的持续更新，每一层的输入分布在训练过程中不断发生变化，迫使后层不断重新适应新的分布，拖慢了训练进程。BN通过在每一层显式地规范化输入分布来缓解这一问题。

**计算过程**：给定一个小批量 $\mathcal{B} = \{\mathbf{z}_1, \ldots, \mathbf{z}_B\}$（$\mathbf{z}_i$ 为第 $i$ 个样本在某层的净输入），BN对每个特征维度 $k$ 独立执行：

$$
\mu_k = \frac{1}{B} \sum_{i=1}^{B} z_{ik}, \quad \sigma_k^2 = \frac{1}{B} \sum_{i=1}^{B} (z_{ik} - \mu_k)^2
$$

$$
\hat{z}_{ik} = \frac{z_{ik} - \mu_k}{\sqrt{\sigma_k^2 + \epsilon}}
$$

$$
y_{ik} = \gamma_k \hat{z}_{ik} + \beta_k
$$

其中 $\gamma_k$ 和 $\beta_k$ 为可学习的缩放和平移参数，用于恢复网络的表示能力——若 $\gamma_k = \sigma_k$, $\beta_k = \mu_k$，则BN变换被完全撤销（恒等变换）。$\epsilon \approx 10^{-5}$ 为数值稳定常数。

**推理阶段**：由于推理时可能不存在小批量或批量大小为1，BN使用训练过程中累积的全局均值和方差的指数移动平均进行归一化。

**BN的效果**：允许使用更大的学习率（损失曲面更加平滑），加速收敛，降低对初始化的敏感性，并提供一定的正则化效果（批量统计量的噪声性质类似于Dropout）。BN已成为CNN训练的标准配置。

### 5.2 层归一化

层归一化（Layer Normalization, LN）（Ba et al., 2016）在每个样本内部跨特征维度进行归一化，而非BN的跨样本归一化：

$$
\mu = \frac{1}{d} \sum_{k=1}^{d} z_k, \quad \sigma^2 = \frac{1}{d} \sum_{k=1}^{d} (z_k - \mu)^2
$$

$$
\hat{z}_k = \frac{z_k - \mu}{\sqrt{\sigma^2 + \epsilon}}, \quad y_k = \gamma_k \hat{z}_k + \beta_k
$$

其中 $d$ 为该层的特征维度。LN的关键优势在于其统计量不依赖于小批量中的其他样本，因此天然适用于批量大小为1或变长序列的场景（如RNN和Transformer）。LN在Transformer架构中是标准的归一化选择（通常采用Pre-LN配置，即在自注意力和前馈层之前施加LN）。

### 5.3 权重归一化

权重归一化（Weight Normalization, WN）（Salimans & Kingma, 2016）不直接归一化层的激活值，而是对权重向量本身进行重参数化。将权重分解为方向和长度两个独立可学习的成分：

$$
\mathbf{w} = g \cdot \frac{\mathbf{v}}{\|\mathbf{v}\|}
$$

其中 $g \in \mathbb{R}$ 为标量长度参数，$\mathbf{v}$ 为方向向量。这种参数化将权重的范数与方向解耦，使得优化器可以独立地调整两者，改善了损失曲面的条件数。

WN的优势在于不引入对小批量统计量的依赖，推理时无需额外操作，计算开销极低。然而，WN通常需要配合数据相关初始化（data-dependent initialization）来确保初始激活值的合理分布。

### 5.4 局部响应归一化

局部响应归一化（Local Response Normalization, LRN）（Krizhevsky et al., 2012）是最早在深度CNN中使用的归一化方法，模拟了生物视觉系统中的侧抑制机制（lateral inhibition）。LRN在相邻通道间对同一空间位置的激活值进行归一化：

$$
y_{c,i,j} = \frac{x_{c,i,j}}{\left(k + \alpha \sum_{c'=\max(0, c-n/2)}^{\min(C-1, c+n/2)} x_{c',i,j}^2\right)^\beta}
$$

其中 $n$ 为归一化窗口大小，$k$、$\alpha$、$\beta$ 为超参数。LRN的直觉是：当某个通道的响应较强时，通过归一化抑制同一位置上相邻通道的响应，促进通道间的竞争。

LRN在AlexNet中发挥了一定作用，但后续研究表明其效果远不如Batch Normalization。VGGNet的实验已经指出LRN对性能提升微乎其微。目前LRN已基本被BN和LN所取代，主要具有历史意义。

## 6 超参数优化

深度学习模型的性能高度依赖于超参数的选择——学习率、批量大小、权重衰减系数、网络层数与宽度、激活函数类型、Dropout率等。超参数优化（Hyperparameter Optimization, HPO）旨在自动化地搜索最优超参数配置。

### 6.1 网格搜索

网格搜索（Grid Search）在每个超参数的预设候选值集合上进行穷举组合搜索。对于 $k$ 个超参数，每个超参数有 $m$ 个候选值，总搜索量为 $m^k$。

网格搜索的优势在于简单、可并行、可复现。然而，它遭受严重的维度灾难——搜索量随超参数维度指数增长。更关键的是，Bergstra和Bengio（2012）指出，网格搜索对不重要的超参数浪费了大量计算资源（在无关维度上重复采样），而对重要超参数的分辨率不足。

### 6.2 随机搜索

随机搜索（Random Search）（Bergstra & Bengio, 2012）从超参数的联合分布中随机采样配置进行评估。Bergstra和Bengio的理论和实验分析表明，当只有少数超参数对性能起决定性作用时（这在实践中几乎普遍成立），随机搜索的效率显著优于网格搜索。

直觉解释是：对于 $k$ 维超参数空间中的 $N$ 次评估，网格搜索在每个维度上仅有 $N^{1/k}$ 个不同的值，而随机搜索在每个维度上有 $N$ 个不同的值。因此，随机搜索在重要维度上具有更高的分辨率。

随机搜索已成为超参数优化的基本基线方法，任何更复杂的方法至少应与其性能相当。

### 6.3 贝叶斯优化

贝叶斯优化（Bayesian Optimization, BO）（Snoek et al., 2012）是一种基于模型的序贯优化方法，在函数评估代价高昂时尤为有效。

BO的核心框架包含两个组件：

**代理模型（Surrogate Model）**：构建超参数配置到模型性能之间的概率映射 $p(y | \mathbf{x})$。常用的代理模型包括高斯过程（Gaussian Process, GP）和树结构Parzen估计器（Tree-structured Parzen Estimator, TPE）。代理模型提供对未评估配置的性能预测及不确定性估计。

**采集函数（Acquisition Function）**：基于代理模型的预测来决定下一个评估点，在利用（exploitation, 选择预测性能好的区域）和探索（exploration, 选择不确定性高的区域）之间取得平衡。常用的采集函数包括期望改进（Expected Improvement, EI）和上置信界（Upper Confidence Bound, UCB）。

BO的迭代过程为：在已评估的 $\{(\mathbf{x}_i, y_i)\}$ 上训练代理模型 → 最大化采集函数确定下一个评估点 → 评估该配置 → 更新代理模型 → 重复。BO通常在10-50次评估内即可找到接近最优的配置。

### 6.4 动态资源分配

动态资源分配策略通过提前终止表现不佳的配置来节省计算资源，使得更多的资源集中在有潜力的配置上。

**逐次减半（Successive Halving）**（Jamieson & Talwalkar, 2016）：从 $N$ 个随机配置开始，每个配置分配少量资源（如少量训练轮次）进行评估；保留表现最好的一半配置，将资源翻倍继续训练；重复此过程直至只剩一个配置。

**Hyperband**（Li et al., 2017）：在逐次减半的基础上进一步解决了"探索-利用"权衡问题——它并行运行多个不同配置的逐次减半实例，每个实例在初始配置数量和每个配置的最小资源之间取不同的权衡。

动态资源分配策略在大规模超参数搜索中极为高效，尤其适合深度学习模型的训练——因为模型在少量训练轮次后的表现往往可以初步预示其最终性能（早期收敛曲线的斜率具有一定的预测性）。

### 6.5 神经架构搜索

神经架构搜索（Neural Architecture Search, NAS）将超参数优化推广到网络结构本身的自动化设计。NAS的目标是自动搜索最优的网络拓扑——层数、每层的操作类型（卷积核大小、步长、空洞率）、连接方式等。

Zoph和Le（2017）首次使用RNN控制器（controller）来生成网络架构描述，并通过REINFORCE算法优化控制器以最大化所生成架构在验证集上的准确率。该工作在CIFAR-10和Penn Treebank上取得了与人类设计架构相当或更优的结果，但搜索过程消耗了800块GPU运行28天。

后续工作显著降低了NAS的计算成本。ENAS（Pham et al., 2018）通过在所有候选架构之间共享权重，将搜索成本降低了1000倍。DARTS（Liu et al., 2019）将离散的架构搜索空间松弛为连续的，使得可以通过梯度下降高效地优化架构参数。这些进展使NAS从学术概念走向了实用技术。

## 7 网络正则化

正则化（Regularization）是指一类通过对学习算法施加约束或惩罚来防止过拟合、提升泛化能力的技术。在深度学习中，由于模型容量极大，正则化尤为重要。

### 7.1 $\ell_1$ 和 $\ell_2$ 正则化

$\ell_p$ 正则化通过在损失函数中添加参数范数惩罚项来约束模型复杂度：

$$
\tilde{\mathcal{L}}(\boldsymbol{\theta}) = \mathcal{L}(\boldsymbol{\theta}) + \lambda \|\boldsymbol{\theta}\|_p^p
$$

**$\ell_2$ 正则化（权重衰减）**：

$$
\Omega(\boldsymbol{\theta}) = \frac{1}{2}\|\boldsymbol{\theta}\|_2^2 = \frac{1}{2}\sum_i \theta_i^2
$$

$\ell_2$ 正则化倾向于将权重推向较小但非零的值，使得损失函数对任何单个权重的依赖程度降低。从贝叶斯视角，$\ell_2$ 正则化等价于对权重施加零均值的高斯先验 $p(\boldsymbol{\theta}) = \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I})$，其中 $\lambda = 1/(2\sigma^2)$。

在SGD中，$\ell_2$ 正则化的参数更新为 $\boldsymbol{\theta} \leftarrow (1 - \eta\lambda)\boldsymbol{\theta} - \eta \nabla \mathcal{L}$，因此也称为"权重衰减"。需要注意，在自适应优化器（如Adam）中，$\ell_2$ 正则化与权重衰减并不等价——AdamW正是为解决这一不一致而提出的。

**$\ell_1$ 正则化**：

$$
\Omega(\boldsymbol{\theta}) = \|\boldsymbol{\theta}\|_1 = \sum_i |\theta_i|
$$

$\ell_1$ 正则化倾向于产生稀疏解——许多权重被精确地推到零。从贝叶斯视角，它对应于Laplace先验。稀疏权重可以被解释为一种自动化的特征选择机制，在某些场景下（如高维低样本问题）具有理论和实际优势。

**弹性网正则化**（Elastic Net）结合了两者：$\Omega = \alpha \|\boldsymbol{\theta}\|_1 + (1-\alpha) \|\boldsymbol{\theta}\|_2^2$。

### 7.2 权重衰减

权重衰减（Weight Decay）在数学形式上与 $\ell_2$ 正则化密切相关，但在现代自适应优化器中两者存在重要区别。

在标准SGD中，$\ell_2$ 正则化梯度 $\nabla \tilde{\mathcal{L}} = \nabla \mathcal{L} + \lambda \boldsymbol{\theta}$ 产生的更新规则与直接对参数施加衰减因子 $(1 - \eta\lambda)$ 完全等价。然而，在Adam等自适应优化器中，$\ell_2$ 正则化梯度 $\lambda \boldsymbol{\theta}$ 会被自适应学习率进一步缩放，导致对不同参数施加不等的有效衰减，破坏了权重衰减的原始语义。

Loshchilov和Hutter（2019）提出的解耦权重衰减直接在参数更新后施加衰减，避免了与自适应学习率的交互，实验表明这种方式在泛化性能上更为稳健。

### 7.3 提前停止

提前停止（Early Stopping）是最简单也最有效的正则化技术之一。其策略为：在训练过程中持续监控模型在验证集上的性能，当验证性能不再改善（或开始下降）时终止训练，并恢复到验证性能最优的参数配置。

$$
\boldsymbol{\theta}^* = \boldsymbol{\theta}^{(t^*)}, \quad t^* = \arg\min_{t} \mathcal{L}_{\text{val}}(\boldsymbol{\theta}^{(t)})
$$

实践中通常设置"耐心"参数（patience）$P$——若连续 $P$ 个评估周期验证性能均未改善，则触发提前停止。

提前停止的正则化机制可从多个角度理解：它限制了优化的有效迭代次数，等价于限制了模型从训练数据中提取信息的总量；在线性模型的特殊情况下，提前停止可以被证明等价于 $\ell_2$ 正则化（Bishop, 1995），其中迭代次数的倒数 $1/t$ 扮演了正则化系数 $\lambda$ 的角色。

### 7.4 丢弃法

丢弃法（Dropout）（Srivastava et al., 2014）是深度学习中最具标志性的正则化技术。在训练阶段，Dropout以概率 $p$（通常取0.5或0.1-0.3）随机将每个神经元的输出置零：

$$
\tilde{h}_j = m_j \cdot h_j, \quad m_j \sim \text{Bernoulli}(1-p)
$$

其中 $m_j$ 为独立同分布的Bernoulli随机变量。每次前向传播都使用一个随机采样的"子网络"，不同的训练样本可能激活不同的子网络。

**推理阶段**：为保持输出的期望值一致，需要对权重进行缩放——将所有权重乘以 $(1-p)$，或等价地，在训练时将未被丢弃的激活值除以 $(1-p)$（称为"Inverted Dropout"，是实践中的标准实现）。

**Dropout的正则化机制**：

从集成学习的角度，Dropout可以被理解为对指数级数量（$2^n$，$n$ 为神经元总数）的子网络进行隐式集成——每次训练使用不同的子网络，推理时使用所有子网络的近似平均。从权重的角度，Dropout迫使每个神经元不能过度依赖特定的其他神经元，必须学习更鲁棒和冗余的特征表示。从噪声注入的角度，Dropout等价于在隐藏层施加乘性噪声，增加了训练目标的随机性。

Gal和Ghahramani（2016）从贝叶斯推断的角度证明了Dropout近似等价于对网络权重的贝叶斯后验进行变分推断，为Dropout提供了严格的概率论基础，并由此发展出了Monte Carlo Dropout——在推理时也保持Dropout活跃并多次采样，以估计模型预测的不确定性。

**变体**：DropConnect（Wan et al., 2013）随机丢弃连接权重而非神经元激活；Spatial Dropout对整个特征图通道进行丢弃，适用于CNN；DropBlock（Ghiasi et al., 2018）丢弃特征图的连续区域。

### 7.5 数据增强

数据增强（Data Augmentation）通过对训练数据施加保持标签不变的随机变换来人为扩大训练集规模，是一种隐式的正则化方法——它增加了训练数据的多样性而不改变标签分布，有助于模型学习对这些变换不变的鲁棒特征。

**图像领域的常见增强**：

几何变换包括随机裁剪（Random Crop）、水平/垂直翻转、随机旋转、仿射变换和弹性变形等。颜色变换包括随机调整亮度、对比度、饱和度和色调，以及颜色抖动（Color Jittering）。遮挡类方法包括Cutout（DeVries & Taylor, 2017）随机遮挡图像的矩形区域，Random Erasing随机擦除图像的任意区域。混合类方法包括Mixup（Zhang et al., 2018）在像素级和标签级同时对两个样本进行线性插值：$\tilde{\mathbf{x}} = \lambda \mathbf{x}_i + (1-\lambda) \mathbf{x}_j$, $\tilde{y} = \lambda y_i + (1-\lambda) y_j$，其中 $\lambda \sim \text{Beta}(\alpha, \alpha)$；CutMix（Yun et al., 2019）将一个图像的矩形区域替换为另一个图像的对应区域，并按面积比例混合标签。

**自动化增强策略**：AutoAugment（Cubuk et al., 2019）使用强化学习搜索最优的增强策略组合；RandAugment（Cubuk et al., 2020）将搜索空间简化为两个超参数（变换数量和变换幅度），以极低的搜索成本获得了与AutoAugment相当的效果。

**NLP领域的数据增强**：包括同义词替换、随机插入/删除/交换词语、回译（Back-Translation，将文本翻译到另一种语言再翻译回来）等。

### 7.6 标签平滑

标签平滑（Label Smoothing）（Szegedy et al., 2016）是一种针对分类任务输出端的正则化技术。标准的交叉熵损失使用one-hot编码的硬标签（如 $[0, 0, 1, 0]$），这要求模型对正确类别产生极高的置信度（Softmax输出趋近于1），导致模型过度自信并可能过拟合。

标签平滑将硬标签替换为软标签：

$$
\tilde{y}_k = (1 - \alpha) y_k + \frac{\alpha}{C}
$$

其中 $\alpha$ 为平滑系数（通常取0.1），$C$ 为类别总数，$y_k$ 为原始的one-hot标签。对于正确类别，软标签为 $1 - \alpha + \alpha/C$；对于错误类别，软标签为 $\alpha/C$。

标签平滑的效果是：阻止模型过度自信地将所有概率质量集中在单一类别上，迫使模型学习更加校准（calibrated）的概率分布。Müller等人（2019）的研究进一步表明，标签平滑使模型学到的特征表示聚类结构更加紧凑——同类样本的表示更加聚集，不同类样本之间的距离更加均匀。

标签平滑已成为训练大规模分类模型的标准技术，在Inception v3、Transformer和ViT等模型中均有应用。

